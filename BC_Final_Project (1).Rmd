---
title: "R_Final_Project"
author: "Gaurav_Kothari, Sanika Moghe, Chaitanya Vyas"
date: "November 15, 2018"
output: html_document
---
##Import Librarires

```{r}
Loadlibraries=function(){
library(rlang)
library(modelr)
library(nycflights13)
library(lubridate)
library(ISLR)
  print("The libraries have been loaded.")
  }
```

#Feature Selection and Data Elimination

```{r}

#Dividing Data into testing and Trainig set with the help of 'sample' function.

Data<-read.csv("bc_data.csv")
bc_new_data<-Data[-1]

set.seed(400)

data_sample <- sample(nrow(bc_new_data), nrow(bc_new_data)*3/4)
data_train <- bc_new_data[data_sample,]
data_test <- bc_new_data[-data_sample,]

#feature selection 

library(corrplot)
library(caret)

#Here first let's find the corelation between different features.
corMatMy <- cor(bc_new_data[2:31])
corrplot(corMatMy, order= "hclust")

#As we can see from the plot that there are number of features which are highly correlated. It is advisable to remove the highly correlated features (for ex. area and radius) to reduce the complexity of models.

#Here, we are removing all the feaures which has correlation of 0.7 or more.

highlyCor <- colnames(data_train[,-1])[findCorrelation(corMatMy, cutoff = 0.7, verbose = TRUE)]

highlyCor

#feature elimination
train_data_cor <- data_train[, which(!colnames(data_train) %in% highlyCor)]

test_data_cor<-data_test[, which(!colnames(data_test)%in% highlyCor)]

#Feature eliminated whole dataset

data_cor<-bc_new_data[, which(!colnames(bc_new_data)%in% highlyCor)]

#After the elimination, there are total 10 most important features 

```

###Exploratory data analysis

```{r}

library(tidyverse)
library(ggplot2)

#Let's check how many of the cases have the Positive cancer result


ggplot(data=Data)+
 geom_bar(mapping=aes(x=diagnosis, fill=diagnosis))

#Relationship between radius_mean and diagnosos

gather(data_cor, x, y, texture_mean:dimension_worst)%>%
ggplot(aes(x=diagnosis, y=y, fill=diagnosis))+
  geom_boxplot()+
  facet_wrap( ~ x, scales = "free", ncol = 3)

#As from all the plots, we can see that area _mean is one of the most important factors in the cancer diognisis.

ggplot(data=data_cor)+
  geom_boxplot(mapping=aes(x=diagnosis,y=area_mean, fill=diagnosis))

```

#Logistic Regression 

```{r}

#Logistic Regression 

lg=glm(diagnosis~texture_mean+area_mean+symmetry_mean+texture_se+symmetry_se+smoothness_se+dimension_se+smoothness_worst+symmetry_worst+dimension_worst, data=train_data_cor, family=binomial)

summary(lg)


#Removing features which has larger p values

lg2=glm(diagnosis~texture_mean+area_mean+dimension_se+smoothness_worst, data=train_data_cor, family=binomial)

summary(lg2)

probability = predict(lg,test_data_cor,type = "response")
pred.glm = rep("B", length(probability))
pred.glm[probability > 0.5] = "M"
with(test_data_cor, table(pred.glm,diagnosis))

ctable_lg <- as.table(matrix(c(83, 4, 2, 54), nrow = 2, byrow = TRUE))
fourfoldplot(ctable_lg, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")

#Accuracy = ((83+54) *100)/143 = 95%

```

#LDA

```{r}

library(MASS)


lda=lda(diagnosis~texture_mean+area_mean+symmetry_mean+texture_se+symmetry_se+smoothness_se+dimension_se+smoothness_worst+symmetry_worst+dimension_worst, data=train_data_cor)

lda

pred.lda = predict(lda, test_data_cor)
with(test_data_cor, table(pred.lda$class,diagnosis))

ctable_lda<- as.table(matrix(c(84, 6, 1, 52), nrow = 2, byrow = TRUE))
fourfoldplot(ctable_lda, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")

#Accuracy = ((84+52)*100/143)= 95%

```

#QDA

```{r}

library(MASS)

qda=qda(diagnosis~., data=train_data_cor)

qda

pred.qda = predict(qda, test_data_cor)
with(test_data_cor,table(pred.qda$class, test_data_cor$diagnosis))


ctable_qda<- as.table(matrix(c(82, 5, 3, 53), nrow = 2, byrow = TRUE))
fourfoldplot(ctable_qda, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")

#Accuracy = ((82+53)*100)/143 = 94%

```

#KNN

```{r}

library(class)

knntrain=train_data_cor[,2:11]
knntest=test_data_cor[,2:11]
knnlabel=train_data_cor[,1]

#KNN with K=1
knn.pred1=knn(knntrain,knntest,knnlabel,k=1)

table(knn.pred1,test_data_cor$diagnosis)

ctable_knn1<- as.table(matrix(c(77, 12, 8, 46), nrow = 2, byrow = TRUE))
fourfoldplot(ctable_knn1, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")

mean(knn.pred1==test_data_cor$diagnosis)

#Accuracy = ((77+46)*100/143) = 86%


#KNN with K=100

knn.pred2=knn(knntrain,knntest,knnlabel,k=100)

table(knn.pred2,test_data_cor$diagnosis)

ctable_knn10<- as.table(matrix(c(81, 15, 4, 43), nrow = 2, byrow = TRUE))
fourfoldplot(ctable_knn10, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")

mean(knn.pred2==test_data_cor$diagnosis)

#Accuracy = ((59+31)*100)/100 = 90%

#As we can see here, logistic regression and LDA has the highest accuracy, followed by QDA. KNN seems to be the least accurate model.

```

#Decision Tree

```{r}

library(rpart)
#?rpart.plot
#View(rpart)
library(rpart.plot)

set.seed(42)
fit <- rpart(diagnosis ~ .,
            data = data_cor,
            method = "class",
            control = rpart.control(xval = 10, 
                                    minbucket = 2, 
                                    cp = 0), 
             parms = list(split = "information"))

rpart.plot(fit, type = 1 , extra = 100, box.palette = c("green","light blue","pink"))

```


